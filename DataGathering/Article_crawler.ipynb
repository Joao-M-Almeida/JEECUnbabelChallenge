{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib2 import HTTPError\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import codecs\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Receives htlm text\n",
    "# Returns a list of all urls that are tagged a and start with 'http://'\n",
    "def find_urls(data):\n",
    "    BS = BeautifulSoup(data)\n",
    "    urls = BS.findAll('a')\n",
    "    url_list = []\n",
    "    for link in urls:\n",
    "        try:\n",
    "            url_list.append(link['href'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "            #print link.attrs\n",
    "    url_list = [url for url in url_list if re.match('http://.*', url, ) != None]\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_article(data):\n",
    "    text = ''\n",
    "    BS = BeautifulSoup(data)\n",
    "    try:\n",
    "        BS=BS.article\n",
    "        if BS == None:\n",
    "            return text\n",
    "        for child in BS.children:\n",
    "            if child.get('itemprop')=='articleBody':\n",
    "                s = child.findAll('p')\n",
    "                for parag in s:\n",
    "                    text = text + parag.get_text() \n",
    "    except AttributeError:\n",
    "        return text  \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_url(url):\n",
    "    m = re.search(\"(https?://[a-zA-Z0-9.\\/]+)\", url)\n",
    "    return m.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_urls(urls, current_url, already_visited = set([])):\n",
    "    to_return = []\n",
    "    for u in urls:\n",
    "        u = clean_url(u)\n",
    "        if u not in already_visited:\n",
    "            domain_lst = u[7:].split('.')\n",
    "            curr_domain = current_url[7:].split('.')\n",
    "            if domain_lst[0] == curr_domain[0] and domain_lst[1] == curr_domain[1]:\n",
    "                to_return.append(u)\n",
    "    return to_return\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def arcticle_crawler(root_url, max_articles = 20):\n",
    "    to_visit = []\n",
    "    already_visited = set([])\n",
    "    found_articles = 0\n",
    "    to_visit.append(root_url)\n",
    "    while to_visit and max_articles-found_articles>0:\n",
    "        #print max_articles\n",
    "        url = to_visit.pop(0)\n",
    "        if url not in already_visited:\n",
    "            try:\n",
    "                site = urllib2.urlopen(url)\n",
    "                text = ''\n",
    "                if site.getcode() == 200:\n",
    "                    data =''\n",
    "                    for l in site:\n",
    "                        data = data+l\n",
    "                    urls = find_urls(data)\n",
    "                    article = extract_article(data)\n",
    "                    if(len(article)>0):\n",
    "                        print \"Found text of size: \" + str(len(article))\n",
    "                        found_articles = found_articles+1\n",
    "                        print url\n",
    "                        with codecs.open('article' + str(found_articles) + '.txt', \"w\", \"utf-8-sig\") as temp:\n",
    "                            temp.write(article)\n",
    "                    to_visit = to_visit + filter_urls(urls, url, already_visited)\n",
    "            except HTTPError:\n",
    "                pass\n",
    "            already_visited.add(url)\n",
    "            print \"Still have \" + str(len(to_visit)) +\" urls to visit, found: \"+str(found_articles)+\" articles.\" \n",
    "\n",
    "    #return already_visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://www.elmundo.es/espana/2016/03/15/56e7ffa922601d0c038b45ca.html'\n",
    "\n",
    "arcticle_crawler('http://www.elmundo.es/espana', max_articles=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
